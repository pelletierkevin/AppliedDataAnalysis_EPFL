{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    " <p><div class=\"lev1\"><a href=\"#Task-A.-Another-LEGO-brick-in-the-wall\"><span class=\"toc-item-num\">Task A.&nbsp;&nbsp;</span>Another LEGO brick in the wall</a></div>\n",
    " <p><div class=\"lev1\"><a href=\"#Task-B.-Drop-the-Bike\"><span class=\"toc-item-num\">Task B.&nbsp;&nbsp;</span>Drop the Bike</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn', Mutes warnings when copying a slice from a DataFrame.\n",
    "\n",
    "# Allows developper to press tab for autocompletion\n",
    "%config IPComplete.greedy=True;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Task A. Another LEGO brick in the wall\n",
    "\n",
    ">LEGO is a popular brand of toy building bricks. They are often sold in sets in order to build a specific object. Each set contains a number of parts in different shapes, sizes and colors. This database contains information on which parts are included in different LEGO sets. It was originally compiled to help people who owned some LEGO sets already figure out what other sets they could build with the pieces they had.\n",
    "\n",
    ">This dataset contains the official LEGO colors, parts, inventories (i.e., sets of LEGO parts which assembled create an object in the LEGO world) and sets (i.e., sets of LEGO inventories which assembled create a LEGO ecosystem). The schema of the dataset can be shown in the following UML diagram: \n",
    "\n",
    ">![lego-schema](lego-schema.png)\n",
    "\n",
    ">In this task you have to apply the following Data Wrangling pipeline:\n",
    ">1. Load your data into `Pandas`\n",
    ">* Explore it and clean its dirty parts\n",
    ">* Use it to answer a set of queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. Loading phase\n",
    ">_Load all the csv files into different `DataFrames`. Use meaningful names for your `DataFrames` (e.g., the respective filenames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGO_DATA_FOLDER = DATA_FOLDER + '/lego'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ Getting a list of all the csv files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allFiles =  ['colors.csv.zip', 'inventories.csv.zip', 'inventory_parts.csv.zip', 'inventory_sets.csv.zip', 'part_categories.csv.zip', 'parts.csv.zip', 'sets.csv.zip', 'themes.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "startingDir = os.getcwd()\n",
    "os.chdir(LEGO_DATA_FOLDER)\n",
    "allFiles = glob.glob( '*.csv.zip' ) # glob finds all the pathnames matching a specified pattern\n",
    "print(\"allFiles = \",allFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Creating a dictionary ``'data'`` with imported dataframes and file names as keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLORS : \n",
      "    id     name     rgb is_trans\n",
      "0  -1  Unknown  0033B2        f\n",
      "1   0    Black  05131D        f\n",
      "2   1     Blue  0055BF        f \n",
      "\n",
      "INVENTORIES : \n",
      "    id  version\n",
      "0   1        1\n",
      "1   3        1\n",
      "2   4        1 \n",
      "\n",
      "INVENTORY_PARTS : \n",
      "    inventory_id   part_id  color_id  quantity is_spare\n",
      "0             1  48379c01        72       1.0        f\n",
      "1             1     48395         7       1.0        f\n",
      "2             1  mcsport6        25       1.0        f \n",
      "\n",
      "INVENTORY_SETS : \n",
      "    inventory_id   set_id  quantity\n",
      "0            35  75911-1         1\n",
      "1            35  75912-1         1\n",
      "2            39  75048-1         1 \n",
      "\n",
      "PART_CATEGORIES : \n",
      "    id            name\n",
      "0   1      Baseplates\n",
      "1   2  Bricks Printed\n",
      "2   3   Bricks Sloped \n",
      "\n",
      "PARTS : \n",
      "        id                                               name  part_cat_id\n",
      "0  0687b1                        Set 0687 Activity Booklet 1           17\n",
      "1    0901  Baseplate 16 x 30 with Set 080 Yellow House Print            1\n",
      "2    0902  Baseplate 16 x 24 with Set 080 Small White Hou...            1 \n",
      "\n",
      "SETS : \n",
      "        id                        name  year  theme_id  num_parts\n",
      "0    00-1             Weetabix Castle   70s       414        471\n",
      "1  0011-2           Town Mini-Figures  1978        84         12\n",
      "2  0011-3  Castle 2 for 1 Bonus Offer  1987       199          2 \n",
      "\n",
      "THEMES : \n",
      "    id            name  parent_id\n",
      "0   1         Technic        NaN\n",
      "1   2  Arctic Technic        1.0\n",
      "2   3     Competition        1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "allFilesNames = list(map(lambda f: f.replace(\".csv.zip\", \"\"), allFiles))\n",
    "data = dict(zip(allFilesNames, list(map(lambda f: pd.read_csv(f), allFiles))))\n",
    "\n",
    "for file, dataframe in data.items(): \n",
    "    print(file.upper(), \": \\n\", dataframe.head(3), \"\\n\")\n",
    "\n",
    "os.chdir(startingDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the original dataframes into variables to separate the processing and better understanding\n",
    "inventory_sets = data['inventory_sets'].copy()\n",
    "inventory_parts = data['inventory_parts'].copy()\n",
    "part_categories = data['part_categories'].copy()\n",
    "sets = data['sets'].copy()\n",
    "colors = data['colors'].copy()\n",
    "themes = data['themes'].copy()\n",
    "inventories = data['inventories'].copy()\n",
    "parts = data['parts'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Cleaning phase\n",
    ">Explore the following columns from your dataset:\n",
    ">1. sets: year\n",
    ">* inventory_parts: quantity\n",
    "\n",
    ">What is the time range of the sets? \n",
    "What is the average quantity of the inventory parts? \n",
    "Do you see any inconsistencies? \n",
    "Provide code that detects and cleans such inconsistencies and validates the coherence of your dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__step 1: cleaning ``sets: year``__\n",
    "\n",
    "> year = Year the set was published"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by analyzing the data we have.\n",
    "\n",
    "Do do that, we check if each entry is valid using a function that tries to wright it in the datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate function to check if a year is in the correct datetime format\n",
    "# return: True if the year is valid, False otherwise\n",
    "def validate_year(year_text):\n",
    "    try:\n",
    "        datetime.datetime.strptime(year_text, '%Y')\n",
    "        return True\n",
    "    except: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries:    11673\n",
      "Number of entries with an invalid year:    3085\n"
     ]
    }
   ],
   "source": [
    "n_total = len(sets['year'])\n",
    "print(\"Total number of entries:   \", n_total) \n",
    "\n",
    "n_not_valid = np.count_nonzero(sets['year'].apply(lambda x : validate_year(x)) == False)\n",
    "print(\"Number of entries with an invalid year:   \", n_not_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid RGBA argument: 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('id', None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6582611b7df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Not valid data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Valid data'\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgraph_colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'lightcoral'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'skyblue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_not_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_total\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_not_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautopct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%1.1f%%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'equal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpie\u001b[0;34m(x, explode, labels, colors, autopct, pctdistance, shadow, labeldistance, startangle, radius, counterclock, wedgeprops, textprops, center, frame, rotatelabels, hold, data)\u001b[0m\n\u001b[1;32m   3336\u001b[0m                      \u001b[0mradius\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounterclock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounterclock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m                      \u001b[0mwedgeprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwedgeprops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtextprops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m                      frame=frame, rotatelabels=rotatelabels, data=data)\n\u001b[0m\u001b[1;32m   3339\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mpie\u001b[0;34m(self, x, explode, labels, colors, autopct, pctdistance, shadow, labeldistance, startangle, radius, counterclock, wedgeprops, textprops, center, frame, rotatelabels)\u001b[0m\n\u001b[1;32m   2900\u001b[0m                                \u001b[0;36m360.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                                \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m                                **wedgeprops)\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0mslices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, center, r, theta1, theta2, width, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \"\"\"\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0mPatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, edgecolor, facecolor, color, linewidth, linestyle, antialiased, hatch, fill, capstyle, joinstyle, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medgecolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;31m# unscaled dashes.  Needed to scale dash patterns by lw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_us_dashes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mset_facecolor\u001b[0;34m(self, color)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \"\"\"\n\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_facecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36m_set_facecolor\u001b[0;34m(self, color)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'patch.facecolor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_facecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid RGBA argument: {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;31m# tuple color.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid RGBA argument: 'id'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADYBJREFUeJzt3HGI33d9x/Hny8ROprWO5QRJou1YuhrKoO7oOoRZ0Y20fyT/FEmguEppwK0OZhE6HCr1rylDELJptolT0Fr9Qw+J5A9X6RAjudJZmpTALTpzROhZu/5TtGZ774/fT++4XHLf3v3uLt77+YDA7/v7fX6/e+fD3TO/fH/3+6WqkCRtf6/a6gEkSZvD4EtSEwZfkpow+JLUhMGXpCYMviQ1sWrwk3wuyXNJnrnC7Uny6SRzSZ5O8rbJjylJWq8hz/A/Dxy4yu13AfvGf44C/7T+sSRJk7Zq8KvqCeBnV1lyCPhCjZwC3pDkTZMaUJI0GTsn8Bi7gQtLjufH1/1k+cIkRxn9L4DXvva1f3TLLbdM4MtLUh9PPvnkT6tqai33nUTws8J1K35eQ1UdB44DTE9P1+zs7AS+vCT1keS/13rfSfyWzjywd8nxHuDiBB5XkjRBkwj+DPDe8W/r3AG8WFWXnc6RJG2tVU/pJPkycCewK8k88FHg1QBV9RngBHA3MAe8BLxvo4aVJK3dqsGvqiOr3F7AX01sIknShvCdtpLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDUxKPhJDiQ5l2QuycMr3P7mJI8neSrJ00nunvyokqT1WDX4SXYAx4C7gP3AkST7ly37O+CxqroNOAz846QHlSStz5Bn+LcDc1V1vqpeBh4FDi1bU8Drx5dvAC5ObkRJ0iQMCf5u4MKS4/nxdUt9DLg3yTxwAvjASg+U5GiS2SSzCwsLaxhXkrRWQ4KfFa6rZcdHgM9X1R7gbuCLSS577Ko6XlXTVTU9NTX1yqeVJK3ZkODPA3uXHO/h8lM29wOPAVTV94DXALsmMaAkaTKGBP80sC/JTUmuY/Si7MyyNT8G3gWQ5K2Mgu85G0m6hqwa/Kq6BDwInASeZfTbOGeSPJLk4HjZQ8ADSX4AfBm4r6qWn/aRJG2hnUMWVdUJRi/GLr3uI0sunwXePtnRJEmT5DttJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNDAp+kgNJziWZS/LwFda8J8nZJGeSfGmyY0qS1mvnaguS7ACOAX8GzAOnk8xU1dkla/YBfwu8vapeSPLGjRpYkrQ2Q57h3w7MVdX5qnoZeBQ4tGzNA8CxqnoBoKqem+yYkqT1GhL83cCFJcfz4+uWuhm4Ocl3k5xKcmClB0pyNMlsktmFhYW1TSxJWpMhwc8K19Wy453APuBO4AjwL0necNmdqo5X1XRVTU9NTb3SWSVJ6zAk+PPA3iXHe4CLK6z5RlX9sqp+CJxj9A+AJOkaMST4p4F9SW5Kch1wGJhZtubrwDsBkuxidIrn/CQHlSStz6rBr6pLwIPASeBZ4LGqOpPkkSQHx8tOAs8nOQs8Dnyoqp7fqKElSa9cqpafjt8c09PTNTs7uyVfW5J+UyV5sqqm13Jf32krSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSE4OCn+RAknNJ5pI8fJV19ySpJNOTG1GSNAmrBj/JDuAYcBewHziSZP8K664H/hr4/qSHlCSt35Bn+LcDc1V1vqpeBh4FDq2w7uPAJ4CfT3A+SdKEDAn+buDCkuP58XW/luQ2YG9VffNqD5TkaJLZJLMLCwuveFhJ0toNCX5WuK5+fWPyKuBTwEOrPVBVHa+q6aqanpqaGj6lJGndhgR/Hti75HgPcHHJ8fXArcB3kvwIuAOY8YVbSbq2DAn+aWBfkpuSXAccBmZ+dWNVvVhVu6rqxqq6ETgFHKyq2Q2ZWJK0JqsGv6ouAQ8CJ4Fngceq6kySR5Ic3OgBJUmTsXPIoqo6AZxYdt1HrrD2zvWPJUmaNN9pK0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqYlDwkxxIci7JXJKHV7j9g0nOJnk6ybeTvGXyo0qS1mPV4CfZARwD7gL2A0eS7F+27Clguqr+EPga8IlJDypJWp8hz/BvB+aq6nxVvQw8ChxauqCqHq+ql8aHp4A9kx1TkrReQ4K/G7iw5Hh+fN2V3A98a6UbkhxNMptkdmFhYfiUkqR1GxL8rHBdrbgwuReYBj650u1VdbyqpqtqempqaviUkqR12zlgzTywd8nxHuDi8kVJ3g18GHhHVf1iMuNJkiZlyDP808C+JDcluQ44DMwsXZDkNuCzwMGqem7yY0qS1mvV4FfVJeBB4CTwLPBYVZ1J8kiSg+NlnwReB3w1yX8mmbnCw0mStsiQUzpU1QngxLLrPrLk8rsnPJckacJ8p60kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNDAp+kgNJziWZS/LwCrf/VpKvjG//fpIbJz2oJGl9Vg1+kh3AMeAuYD9wJMn+ZcvuB16oqt8HPgX8/aQHlSStz5Bn+LcDc1V1vqpeBh4FDi1bcwj4t/HlrwHvSpLJjSlJWq+dA9bsBi4sOZ4H/vhKa6rqUpIXgd8Ffrp0UZKjwNHx4S+SPLOWobehXSzbq8bci0XuxSL3YtEfrPWOQ4K/0jP1WsMaquo4cBwgyWxVTQ/4+tuee7HIvVjkXixyLxYlmV3rfYec0pkH9i453gNcvNKaJDuBG4CfrXUoSdLkDQn+aWBfkpuSXAccBmaWrZkB/mJ8+R7g36vqsmf4kqSts+opnfE5+QeBk8AO4HNVdSbJI8BsVc0A/wp8Mckco2f2hwd87ePrmHu7cS8WuReL3ItF7sWiNe9FfCIuST34TltJasLgS1ITGx58P5Zh0YC9+GCSs0meTvLtJG/Zijk3w2p7sWTdPUkqybb9lbwhe5HkPePvjTNJvrTZM26WAT8jb07yeJKnxj8nd2/FnBstyeeSPHel9ypl5NPjfXo6ydsGPXBVbdgfRi/y/hfwe8B1wA+A/cvW/CXwmfHlw8BXNnKmrfozcC/eCfz2+PL7O+/FeN31wBPAKWB6q+fewu+LfcBTwO+Mj9+41XNv4V4cB94/vrwf+NFWz71Be/GnwNuAZ65w+93Atxi9B+oO4PtDHnejn+H7sQyLVt2Lqnq8ql4aH55i9J6H7WjI9wXAx4FPAD/fzOE22ZC9eAA4VlUvAFTVc5s842YZshcFvH58+QYuf0/QtlBVT3D19zIdAr5QI6eANyR502qPu9HBX+ljGXZfaU1VXQJ+9bEM282QvVjqfkb/gm9Hq+5FktuAvVX1zc0cbAsM+b64Gbg5yXeTnEpyYNOm21xD9uJjwL1J5oETwAc2Z7RrzivtCTDsoxXWY2Ify7ANDP57JrkXmAbesaETbZ2r7kWSVzH61NX7NmugLTTk+2Ino9M6dzL6X99/JLm1qv5ng2fbbEP24gjw+ar6hyR/wuj9P7dW1f9t/HjXlDV1c6Of4fuxDIuG7AVJ3g18GDhYVb/YpNk222p7cT1wK/CdJD9idI5yZpu+cDv0Z+QbVfXLqvohcI7RPwDbzZC9uB94DKCqvge8htEHq3UzqCfLbXTw/ViGRavuxfg0xmcZxX67nqeFVfaiql6sql1VdWNV3cjo9YyDVbXmD426hg35Gfk6oxf0SbKL0Sme85s65eYYshc/Bt4FkOStjIK/sKlTXhtmgPeOf1vnDuDFqvrJanfa0FM6tXEfy/AbZ+BefBJ4HfDV8evWP66qg1s29AYZuBctDNyLk8CfJzkL/C/woap6fuum3hgD9+Ih4J+T/A2jUxj3bccniEm+zOgU3q7x6xUfBV4NUFWfYfT6xd3AHPAS8L5Bj7sN90qStALfaStJTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ18f+GmWq6NWLIwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = 'Not valid data', 'Valid data'; graph_colors = ['lightcoral', 'skyblue']\n",
    "plt.pie([n_not_valid, n_total-n_not_valid], autopct='%1.1f%%', explode=(0.1, 0), labels=labels, colors=graph_colors)\n",
    "plt.axis('equal'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first look at the entries in ``sets['year']`` shows us that __we have a lot of incorrect year formats: 26.4%.__ We understand from this result that we will need to correct as much as we can to avoid losing too many informations.\n",
    "\n",
    "Let's take a look a the different entries we have in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique entries:   \",len(sets['year'].unique()))\n",
    "print(\"\\n\",sets['year'].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the repartition of the correct data. That way, we will be able to check if a corrected year seems to fit our existing data or if we introduced outliers due to incorrect correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets[sets['year'].apply(lambda x : validate_year(x))]['year'].apply(lambda x : float(x)).hist(color = \"skyblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first quick look in the list of years we have in our data shows that quite __a lot of them are negative numbers__, and thus incorect year formats. (Legos are old, but not that much!)\n",
    "\n",
    "We make the following hypothesis: __These data most probably correspond to typos, and a negative sign came up in front of the real value.__\n",
    "\n",
    "We believe our assumption to be justified, but there is always a chance that we may be mistaken and that our analysis may be incorrect. So before making a change, let's check how many values are affected by this same error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_negative = len(sets.loc[sets['year'].str.contains(\"-\")])             \n",
    "print(\"Number of entries with a negative year:   \", n_negative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replacing \"negative\"values by deleting all \"-\"\"\n",
    "sets['year'] = sets['year'].apply(lambda x: x.replace(\"-\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check our corrected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique entries:   \",len(sets['year'].unique()))\n",
    "n_not_valid = np.count_nonzero(sets['year'].apply(lambda x : validate_year(x)) == False)\n",
    "print(\"\\nNumber of entries with an invalid year:   \", n_not_valid)\n",
    "plt.pie([n_not_valid, n_total-n_not_valid], autopct='%1.1f%%', explode=(0.1, 0), labels=labels, colors=graph_colors)\n",
    "plt.axis('equal'); plt.show()\n",
    "\n",
    "sets[sets['year'].apply(lambda x : validate_year(x))]['year'].apply(lambda x : float(x)).hist(color = \"skyblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only by replacing negative values by positive ones, we fixed 1179 incorrect years to a valid one! Indeed, the histogram shows that we've not added any outlier to our data.\n",
    "\n",
    "Let's now ckeck all remaining incorrect years we hhave in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sets['year'][sets['year'].apply(lambda x : validate_year(x)) == False].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can see at a glance that a pattern seems to emerge in the remaining incorrect years: __Many years have 5 digits, and the last digit is always double!__\n",
    "\n",
    "So let's make a new hypothesis: __Once again, these dates correspond to typos, and the last digit has been typed a second time after the correct value for the year.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting last number if years consist of 5 digits\n",
    "sets['year'] = sets['year'].apply(lambda x: x[:-1] if len(x)>4 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can check our results once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique entries:   \",len(sets['year'].unique()))\n",
    "n_not_valid = np.count_nonzero(sets['year'].apply(lambda x : validate_year(x)) == False)\n",
    "print(\"\\nNumber of entries with an invalid year:   \", n_not_valid)\n",
    "plt.pie([n_not_valid, n_total-n_not_valid], autopct='%1.1f%%', explode=(0.1, 0), labels=labels, colors=graph_colors)\n",
    "plt.axis('equal'); plt.show()\n",
    "sets[sets['year'].apply(lambda x : validate_year(x))]['year'].apply(lambda x : float(x)).hist(color = \"skyblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second correction we applied was very effective! We managed to correct 1792 years without adding any outlier. Only 114 invalid years remain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ckeck again all remaining incorrect years we hhave in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sets['year'][sets['year'].apply(lambda x : validate_year(x)) == False].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, __we only have 2 incorrect unique entries, but it doesn't seem we could apply any correction to them.__ Indeed, this errors come from users who entered an approximate year instead of a precise value, as 70s for example coult potentially be any year between 1970 and 1979.\n",
    "\n",
    "This erros represent only 1.0% of our data, so __we consider that deleting this data is highly acceptable and will only make us loose very little of the information.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dropping all remaining rows with pandas.DataFrame.drop\n",
    "sets.drop( sets[ sets.year.apply(lambda y : not validate_year(y)) ].index, inplace=True)\n",
    "print(sets['year'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check out data one last time to see if we still ave any incorrect year remaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_not_valid = np.count_nonzero(sets['year'].apply(lambda x : validate_year(x)) == False)\n",
    "print(\"\\nNumber of entries with an invalid year:   \", n_not_valid)\n",
    "plt.pie([n_not_valid, n_total-n_not_valid], autopct='%1.1f%%', explode=(0.1, 0), labels=labels, colors=graph_colors)\n",
    "plt.axis('equal'); plt.show()\n",
    "sets[sets['year'].apply(lambda x : validate_year(x))]['year'].apply(lambda x : float(x)).hist(color = \"skyblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any more incorrect data, meaning that __all of our entries in 'year' can be converted to datatime format!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__step 2: cleaning ``inventory_parts: quantity``__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by analyzing the data we have.\n",
    "\n",
    "This time, we are analyzing a quantity of parts included in a Lego set. We can assume that this number can be very large, but needs to be at least superior or equal to 1. Indeed, if a part belongs to a set, it wouldn't make sense for its quantity to be 0. Moreover, a quantity is by definition a positive number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = len(inventory_parts['quantity'])\n",
    "print(\"Total number of entries:   \", n_total) \n",
    "\n",
    "n_not_valid = np.count_nonzero(inventory_parts['quantity'].apply(lambda x : x<1) == True)\n",
    "print(\"Number of entries with an invalid year:   \", n_not_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.pie([n_not_valid, n_total-n_not_valid], autopct='%1.1f%%', explode=(0.1, 0), labels=labels, colors=graph_colors)\n",
    "plt.axis('equal'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of invalid data!\n",
    "\n",
    "First let's see the repartition of the correct data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correct_quantities = inventory_parts[inventory_parts['quantity'].apply(lambda x : x>0)]['quantity'].apply(lambda x : float(x))\n",
    "print(correct_quantities.describe())\n",
    "correct_quantities.hist(color=\"skyblue\",bins=np.arange(20)); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the incorrect entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_parts[inventory_parts['quantity'].apply(lambda x : x<1) == True]['quantity'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems like __we only have one type of errors this time.__ Some quantities are set to \"-inf\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier, we must have `quantity > 0` for it to be correct.\n",
    "\n",
    "Considering the important numbers of unset quantities, and using the Lego informations given by the repartition of the correct data telling us that there are a lot more unique pieces in a set, we make the following hypothesis: __Every piece which has a quantity of -inf is a unique piece and we will set it to 1.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing -inf with 1\n",
    "inventory_parts.loc[inventory_parts['quantity'] < 0, 'quantity'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results, and verify that there is no remaining errors in the quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inventory_parts['quantity'].describe())\n",
    "n_not_valid = np.count_nonzero(inventory_parts['quantity'].apply(lambda x : x<1) == True)\n",
    "print(\"\\nNumber of entries with an invalid year:   \", n_not_valid) \n",
    "plt.pie([n_not_valid, n_total-n_not_valid], autopct='%1.1f%%', explode=(0.1, 0), labels=labels, colors=graph_colors)\n",
    "plt.axis('equal'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the quantities are correct! The dataset in now cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Querying phase\n",
    "Answer the following queries using the functionality of `Pandas`:\n",
    "\n",
    "1. List the ids of the inventories that belong to sets that contain cars. (*Hint: Find a smart way to distinguish which sets contain cars based on the sets' name*).\n",
    "* Plot the distribution of part categories as a (horizontal) bar chart. Restrict yourself to the 20 largest part categories (in terms of the number of parts belonging to the category).\n",
    "* Find the dominant color of each set. Then, plot using a (horizontal) bar chart, the number of sets per dominant color. Color each bar with the respective color that it represents.\n",
    "* Create a scatter plot of the *luminance*\\* of the sets vs their publishing year. What do you observe for the years 1980-1981? How do you interpret what you see?\n",
    "\n",
    "\\*The luminance of a color is a [measure of brightness](https://en.wikipedia.org/wiki/Luminance) which, given its RGB representation, can be computed as follows:\n",
    "\n",
    "$luminance = \\sqrt{0.299*R^2 + 0.587*G^2 + 0.114*B^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "__Question A3.1 : List the ids of the inventories that belong to sets that contain cars.__\n",
    "\n",
    "We can see that themes contains the names of all different lego themes. So after looking at some values, we highlight \n",
    "that some themes can be link to cars, such as 'Race' and 'Traffic'.\n",
    "\n",
    " - So first step, query theme ids that matches with a list = ['Race', 'Traffic']\n",
    " - Then sets dataFrame contains ids and theme_ids. So we query ids in sets that match with theme_id\n",
    " - Finally we get inventory id that match with previous sets ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word of car theme found in theme\n",
    "car_theme = ['Race', 'Traffic']\n",
    "\n",
    "# Query ids in theme link to car\n",
    "car_theme_id = themes['id'].loc[themes['name'].isin(car_theme)].values\n",
    "\n",
    "# Query ids in sets of id theme link to car\n",
    "sets_car_ids = sets['id'].loc[sets['theme_id'].isin(car_theme_id)].values\n",
    "\n",
    "inventoriy_contains_car_id = inventory_sets['inventory_id'].loc[inventory_sets['set_id'].isin(sets_car_ids)].values;\n",
    "print(set(inventoriy_contains_car_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question A3.2 : Plot the distribution of part categories as a (horizontal) bar chart.__\n",
    "\n",
    "First we count all parts_cat_id in parts is a new list Then we merge this list (which counts part_cat_id in parts) with part_categories to get the name, count and id of each part_cat_id.\n",
    "Then we sort the table descending and keep the 20 first that we display in a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couunt all part_cat_id in parts\n",
    "distribution = parts.groupby(['part_cat_id']).count();\n",
    "\n",
    "part_cat_id = part_categories.set_index('id');\n",
    "\n",
    "# Merge part_cat_id with their respective names based on ids\n",
    "distribution_part = pd.merge(part_cat_id, distribution, left_index=True, right_index=True);\n",
    "distribution_part = distribution_part.drop(columns='name_y');\n",
    "\n",
    "# Change column names\n",
    "distribution_part = distribution_part.rename(columns={'name_x': 'name', 'id': 'count'})\n",
    "\n",
    "# Sort descending values of count\n",
    "distribution_part = distribution_part.sort_values('count',ascending=False);\n",
    "# Keep the 20 first values\n",
    "distribution_part = distribution_part.head(20)\n",
    "\n",
    "bar_plot = distribution_part.plot.bar(rot=0, color=graph_colors[1])\n",
    "bar_plot.set_ylabel(\"Quantity\");\n",
    "bar_plot.set_xlabel(\"Part categories IDs\");\n",
    "\n",
    "distribution_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question A3.3 : Find the dominant color of each set__\n",
    "\n",
    "To find the dominant color of a set, we must:\n",
    "- select each set\n",
    "- find every inventories belonging to it\n",
    "- get every parts in those inventories and their quantities\n",
    "- finally we can compute which part color is the most dominant in the set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_sets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inventory_id values in inventory_set\n",
    "inventory_sets_inventory_id = set(inventory_sets.inventory_id.values)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Find most common element in a list lst (found online)\n",
    "def most_common(lst):\n",
    "    return max(lst, key=Counter(lst).get)\n",
    "\n",
    "# groupby with agg allow to groupby a column using the operation provided (Function to use for aggregating the data)\n",
    "# to_frame : convert serie to dataFrame\n",
    "inventory_part_color = inventory_parts.groupby('inventory_id').color_id.agg(most_common).to_frame().reset_index()\n",
    "\n",
    "inventory_set_color = inventory_sets.groupby('set_id').inventory_id.agg(most_common).to_frame().reset_index()\n",
    "\n",
    "\n",
    "# Match color_id with set_id\n",
    "color_set = inventory_part_color[['inventory_id','color_id']].\\\n",
    "            merge(inventory_set_color[['inventory_id','set_id']])#, left_on='inventory_id', right_on='inventory_id')\n",
    "\n",
    "color_set = color_set.merge(colors,how='left', left_on='color_id', right_on='id')\n",
    "\n",
    "\n",
    "\n",
    "dominant_color_set = color_set.groupby('name').agg({'set_id': 'count', 'rgb': 'max'})\n",
    "\n",
    "color_bar = dominant_color_set.plot.bar(rot=90, legend=False,\n",
    "                                        color=[['#' + rgb for rgb in dominant_color_set['rgb'].values]],\n",
    "                                        edgecolor = 'black');\n",
    "color_bar.set_ylabel = ('Set number');\n",
    "color_bar.set_xlabel = ('Colors');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question A3.4 : Create a scatter plot of the *luminance*\\* of the sets vs their publishing year.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Task B. Drop the bike\n",
    "\n",
    "*Los Angeles Metro* has been sharing publicly [anonymized *Metro Bike Share* trip data](https://bikeshare.metro.net/about/data/) under the [Open Database License (ODbL)](http://opendatacommons.org/licenses/odbl/1.0/).\n",
    "\n",
    "In this task you will again perform data wrangling and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Loading phase\n",
    "Load the json file into a `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIKES_DATA_FOLDER = DATA_FOLDER + '/bikes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the json data we use the function read_json implemented by pandas which will store it directly in a dataframe.Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bikes = pd.read_json(BIKES_DATA_FOLDER+'/metro-bike-share-trip-data.json.zip' )\n",
    "print(data_bikes.shape)\n",
    "data_bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the DataFrame containing 132427 samples of 14 columns parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Cleaning phase\n",
    "Describe the type and the value range of each attribute. Indicate and transform the attributes that are `Categorical`. Are there redundant columns in the dataset (i.e., are there columns whose value depends only on the value of another column)? What are the possible pitfalls of having such columns? Reduce *data redundancy* by extracting such columns to separate `DataFrames`. Which of the two formats (the initial one or the one with reduced data redundancy) is more susceptible to inconsistencies? At the end print for each `Dataframe` the *type of each column* and it's *shape*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the two formats (the initial one or the one with reduced data redundancy) is more susceptible to inconsistencies? ==> initial one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to describe the type and the range value for each attribute.\n",
    "We'll do a loop to iterate in each attribute and print all these informations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of the DataFrame\n",
    "print(\"Shape data : \"+str(data_bikes.shape))\n",
    "# Print the type of each attribute\n",
    "print(data_bikes.dtypes)\n",
    "for i in data_bikes.columns:\n",
    "    print(\"\\n\" + str(i) +\" : \") # Print the name of the parameter and its type\n",
    "    # Print the max and min values to get information about the range\n",
    "    print(\"min : \" + str(min(data_bikes[i])) ) \n",
    "    print(\"max : \" + str(max(data_bikes[i])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now analyze and have a better view of the data.\n",
    "\n",
    "\n",
    "\n",
    "<b>Bike ID</b> : numpy.float64\n",
    "min : 1349.0\n",
    "max : 6728.0\n",
    " => Represent an ID (which could have been integer because these are real numbers)\n",
    "\n",
    "<b>Duration</b> : numpy.int64\n",
    "min : 60\n",
    "max : 86400\n",
    " => Represent the duration of the trip, End time - Start Time, and the unit is the second\n",
    " \n",
    "<b>End Time</b> : 'str' (Should apply a transformation into datetime)\n",
    "min : 2016-07-07T04:20:00\n",
    "max : 2017-04-02T10:32:00\n",
    "=> Date type YYYY-MM-DDTHH:MM:SS\n",
    "\n",
    "<b>Ending Station ID</b> : numpy.float64'\n",
    "min : 3000.0\n",
    "max : 4108.0\n",
    "=> Represent an ID \n",
    "\n",
    "<b>Ending Station Latitude</b> : numpy.float64\n",
    "min : 0.0\n",
    "max : 34.0642815\n",
    "\n",
    "<b>Ending Station Longitude</b> : numpy.float64\n",
    "min : -118.472832\n",
    "max : 0.0\n",
    "\n",
    "<b>Passholder Type</b> : 'str' (Should apply a transformation into Category)\n",
    "min : Flex Pass\n",
    "max : Walk-up\n",
    "=> 4 categories data : [Monthly Pass, Flex Pass, Walk-up, Staff Annual]  / Related to Plan Duration\n",
    "\n",
    "<b>Plan Duration</b> : 'numpy.float64'\n",
    "min : 0.0\n",
    "max : 365.0\n",
    "=> Represents the number of days related to the Passholder Type [30, 365, 0, nan]\n",
    "\n",
    "<b>Start Time</b> : 'str' (Should apply a transformation into datetime)\n",
    "min : 2016-07-07T04:17:00\n",
    "max : 2017-03-31T23:45:00\n",
    "=> Date type YYYY-MM-DDTHH:MM:SS\n",
    "\n",
    "<b>Starting Station ID</b> : numpy.float64\n",
    "min : 3000.0\n",
    "max : 4108.0\n",
    "=> Represent an ID \n",
    "\n",
    "<b>Starting Station Latitude</b> : numpy.float64\n",
    "min : 0.0\n",
    "max : 34.0642815\n",
    "\n",
    "<b>Starting Station Longitude</b> : numpy.float64\n",
    "min : -118.472832\n",
    "max : 0.0\n",
    "\n",
    "<b>Trip ID</b> : numpy.int64\n",
    "min : 1912818\n",
    "max : 23794218\n",
    "\n",
    "<b>Trip Route Category</b> : 'str' (Should apply a transformation into Category)\n",
    "min : One Way\n",
    "max : Round Trip\n",
    "=> 2 categories data [One Way, Round Trip] / Related to Ending/Starting Station ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Categories </h3>\n",
    "Indicate and transform the attributes that are `Categorical`. \n",
    "\n",
    "Trip Route Category and Passholder Type should be categorical, because they are strings limited to a few values and are describing obviously a category.\n",
    "\n",
    "Let's analyze the form and the content values of these atttributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bikes['Trip Route Category'].unique()\n",
    "#Check the possible values for 'Trip Route Category' and its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bikes['Passholder Type'].unique()\n",
    "#Check the possible values for 'Passholder Type' and its type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the 2 attributes are related to an array of string.\n",
    "In the next operations we'll transform these columns into categorical ones.\n",
    "\n",
    "We'll start by turning the __[Trip Route Category]__ into a categorical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze the attribute, display the differents possible values and finally print \n",
    "#the number of categories in this attribute. This is an informative line\n",
    "print(\"Categorical analysis of [Trip Route Category]: \\n\"+ str(pd.Categorical(data_bikes['Trip Route Category'])))\n",
    "\n",
    "#We will now replace inplace the attribute Trip Route Category to a Category attribute instead of strings.\n",
    "data_bikes['Trip Route Category'] = data_bikes['Trip Route Category'].astype('category')\n",
    "\n",
    "# Print the description of the Trip Route Category attribute to see that it is now a Categorical column.\n",
    "print(\"\\nDescription of [Trip Route Category] \\n\" + str(data_bikes['Trip Route Category'].describe()))\n",
    "\n",
    "data_bikes['Trip Route Category'].unique()\n",
    "#Check the possible values for 'Trip Route Category' and its type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the [Trip Route Category] is now referenced as Categories instead of an array of string.\n",
    "\n",
    "We'll then do the same procedure for __[Passholder Type]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical analysis of [Passholder Type]: \\n\"+ str(pd.Categorical(data_bikes['Passholder Type'])))\n",
    "data_bikes['Passholder Type'] = data_bikes['Passholder Type'].astype('category')\n",
    "print(\"\\nDescription of [Passholder Type]  \\n\" + str(data_bikes['Passholder Type'].describe()))\n",
    "\n",
    "data_bikes['Passholder Type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Redundancy </h3>\n",
    "We can see that the Plan Duration category is redundent because it is completely linked with the Passeholder Type category. Additionally, we can delete the Trip Route Category because it is related to the check of Starting/Ending Station ID. If it is the same station => Round Trip, if not => One way. Finally the Duration is linked to the End time - Start time so we can get rid of it.\n",
    "\n",
    "Redundent columns : \n",
    "- Duration = End time - Start time (converted in seconds)\n",
    "- Passholder type = funct(Plan Duration) [Passholder Type]=[Plan Duration] :\n",
    "    - [Monthly Pass] = [30]\n",
    "    - [Flex Pass]= [365]\n",
    "    - [Walk-up] = [0]\n",
    "    - [Staff Annual] = [nan]\n",
    "- Trip route category = funct(Starting Station ID, Ending station ID) :\n",
    "    If the Starting Station ID is the same as the Ending Station ID then the 'Trip Route Category' is [Round Trip], otherwise it is [One Way]\n",
    "    \n",
    "    \n",
    "What are the possible pitfalls of having such columns? \n",
    "\n",
    "Having such redundent columns will add some not necessarily space and increase the size of the database. Indeed with N (the number of samples) which could be big, adding a column can be expensive. Data redundancy can lead to anomalies, and corruption : Indeed if we want to add some additional samples, there could be some errors during the insertion. For example, if someone made a mistake and put a duration of 40sec instead of 400, it would not happen (less) if we calculate manually the data : End Time - Start Time. \n",
    "\n",
    "Reduce data redundancy by extracting such columns to separate DataFrames. Which of the two formats (the initial one or the one with reduced data redundancy) is more susceptible to inconsistencies? At the end print for each Dataframe the type of each column and it's shape.\n",
    "\n",
    "We will now reduce this data redudancy by dropping the found redundant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily drop the selected columns using the drop method to our DataFrame\n",
    "new_data_bikes = data_bikes.drop(columns=[\"Trip Route Category\", \"Duration\", \"Plan Duration\"])\n",
    "\n",
    "# Print the type of each attributes\n",
    "print(\" ----- Old Data, Shape :\" + str(data_bikes.shape))\n",
    "print(data_bikes.dtypes)\n",
    "    \n",
    "print(\"\\n\\n ----- New Data, Shape :\" + str(new_data_bikes.shape))\n",
    "print(new_data_bikes.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then see that the 3 columns are removed in the new one, and that we have effectively our Categorical attributes.\n",
    "\n",
    "Below are printed some additional information about the memory usage difference between the 2 dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memory Usage of old DataFrame : \" + str(sum(data_bikes.memory_usage())))\n",
    "print(\"Memory Usage of new DataFrame : \" + str(sum(new_data_bikes.memory_usage())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Querying phase\n",
    "Answer the following queries using the functionality of `Pandas`.\n",
    "\n",
    "1. Plot the *distribution* of the number of outgoing trips from each station in a histogram with 20 bins (Hint: each bin describes a range of counts, not stations).\n",
    "* Plot histograms for the *duration* and *trip starting hour in the day* attributes. For both the *duration*  and the *trip starting hour* use *discrete 1-hour intervals*. What do you observe in each plot? What are some popular values in the *duration* plot? Explain the local maxima and the trends you observe on the *trip starting hour* plot based on human behavior.\n",
    "* For each *trip route category*, calculate the proportion of trips by *passholder type* and present your results in *a stacked bar chart with normalized height*.\n",
    "* Considering only trips that begin in the morning hours (before noon), plot in *a single bar chart* the proportion of trips by *passholder type* and *trip route category*. Explain any outliers you observe.\n",
    "* Separate the hours of the day into two intervals that have (approximately) the same number of bikes leaving the stations. For each of the two intervals calculate the proportion of trips by *passholder type* and *trip route category*. Present your results in a `DataFrame` which has a unique, non-composite index. Does the proportion of trips depend on whether it is the first or second hour interval? Would the company have any significant benefit by creating a more complex paying scheme where monthly pass users would pay less in the first interval and (equally) more on the second one? Assume that the number of trips per interval will not change if the scheme changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1)__ Plot the distribution of the number of outgoing trips from each station in a histogram with 20 bins (Hint: each bin describes a range of counts, not stations).\n",
    "\n",
    "For the querying phase, we will use the initial dataframe. Indeed the redundants columns are used for mainly all of these. Since they are already well implemented it is better to use them. \n",
    "\n",
    "To create this distribution we will have to count for each station the number of \"One Way\" Trip Category because we only want the outgoing trips.\n",
    "For this we will first of all create a crosstab merging the [\"Starting Station ID\"] and [\"Trip Route Category\"] attributes.  It will automatically give the amount of One Way and Round Trip for each stations.\n",
    "\n",
    "Then, since we only want the \"One Way\" trips, we'll extract from this dataframe the column represented by \"One Way\". \n",
    "\n",
    "We are now able to construct the Serie composed of the extracted counts, indexed by the Stations ID.\n",
    "\n",
    "Then we plot the histogram with 20 bins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the cross tab merging the [\"Starting Station ID\"] and [\"Trip Route Category\"]\n",
    "# It gives automatically the count for each attributes.\n",
    "cross_tab_stationsID = pd.crosstab(data_bikes[\"Starting Station ID\"], data_bikes[\"Trip Route Category\"])\n",
    "\n",
    "# We extract the counts only for the \"One Way\" value\n",
    "number_outgoing = cross_tab_stationsID.iloc[:][\"One Way\"]\n",
    "\n",
    "print(cross_tab_stationsID.head())\n",
    "\n",
    "# Creation of the serie which will be implemented to plot the histogram\n",
    "data = pd.Series(number_outgoing, index=cross_tab_stationsID.index)\n",
    "print(data.head())\n",
    "\n",
    "#Plot the histogram of the extracted data\n",
    "data.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2)__ Plot histograms for the duration and trip starting hour in the day attributes. For both the duration and the trip starting hour use discrete 1-hour intervals. What do you observe in each plot? What are some popular values in the duration plot? Explain the local maxima and the trends you observe on the trip starting hour plot based on human behavior.\n",
    "\n",
    "__Duration Histogram__\n",
    "\n",
    "To creat the histogram of durations we have to extract it and transform it into hours unit from minuts (so we divide by 60). \n",
    "Then we simply have to plot the histogram of this extracted data. For the parameters we want 1h per interval so we'll creat a vector from 0 to 24 included (so to 25 with Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Duration data, and transform it from seconds to hours\n",
    "durations = data_bikes['Duration']/60\n",
    "\n",
    "# Plot the histogram and put the bins parameter with a vector [0,1 ..., 24]\n",
    "durations.hist(bins=np.arange(24+1), alpha=0.7)\n",
    "\n",
    "# Store the value of each bin and intervals\n",
    "count, division = np.histogram(durations, bins=np.arange(24+1))\n",
    "print(count)\n",
    "print(division)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the print that we have indeed the values for the histogram with 1h per bins. \n",
    "[0->1], [1->2] etc..\n",
    "\n",
    "Then we can clearly observe that the majority of the trips have a duration of less than 1 hour. The data is not very sparsed and is all concentrated in the 3-4 first hours. Generally people do not bike more than 1 hour, especially in town where it is used for small distances. There is some occurences for the bin [23h => 24h] which means that they give the bike back after 24h. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hour Time Histogram__\n",
    "\n",
    "For this we want to extract from the 'Start Time' attribute the hour information. \n",
    "We then first have to transform the 'Start Time' and 'End Time' data from string to a datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, time\n",
    "\n",
    "# Execute it only once ///\n",
    "# Transform the string date from 'Start Time' to a datetime type\n",
    "data_bikes['Start Time'] = data_bikes['Start Time'].apply(lambda d: datetime.strptime(d, '%Y-%m-%yT%H:%M:%S'))\n",
    "\n",
    "# Transform the string date from 'End Time' to a datetime type\n",
    "data_bikes['End Time'] = data_bikes['End Time'].apply(lambda d: datetime.strptime(d, '%Y-%m-%yT%H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the datetime type we are able to extract the hour information for these attributes using dt.hour.\n",
    "We well then extract the data and and apply the function to plot the histogram using the same approach to set the bins equivalent to 1 hour each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 'Hours' number of trip histogram, with 1h per bin\n",
    "(data_bikes['Start Time'].dt.hour).hist(bins=np.arange(24+1), alpha=0.7)\n",
    "\n",
    "# Store and print the informations about the histograms to check the\n",
    "# value for each bin\n",
    "hist_starttime, division_starttime = np.histogram((data_bikes['Start Time'].dt.hour), bins=np.arange(24+1))\n",
    "print(hist_starttime)\n",
    "print(division_starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this histogram  we can see that there is a lot of trip between 11am to 6pm. This is based on the human behaviour so we obsviously have a lot of trips during the normal day, then a very few during the night and early in the morning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3)__ For each trip route category, calculate the proportion of trips by passholder type and present your results in a stacked bar chart with normalized height.\n",
    "\n",
    "We now want to count the number of trips by passholder type for each trip route category (One way, Round trip).\n",
    "So we'll create a crosstab to merge [\"Trip Route Category\"] and [\"Passholder Type\"]. It will then give the number of occurences for each combination. But we want a normalized height so we'll add a parameter in the 'crosstab' method. This normalization should not be on all the dataframe, we only want to normalize each index (One Way and Round Trip) to see the percentages of trips. Finally we'll plot the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and store the normalized crosstab with attributes \"Trip Route Category\" and \"Passholder Type\".\n",
    "cross_route_passolder = pd.crosstab(data_bikes[\"Trip Route Category\"], data_bikes[\"Passholder Type\"], normalize='index')\n",
    "\n",
    "#Print the dataframe to havea view of the data, and watch the percentages and normalization.\n",
    "print(cross_route_passolder)\n",
    "\n",
    "# Plot the stacked bar using the stored data extracted\n",
    "cross_route_passolder.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe on the printed data, that we have accurately the right normalization on each row (Passholder Type).\n",
    "Furthermore the stacked plot show this extracted data with success. \n",
    "We can then observe that there is mostly 'Monthly Pass' for the 'One Way' Route Category, and mostly 'Walk-up' for the 'Round Trip' category. We can think that for the 'One Way' route category, these are people using frequently the bike to go to work or others. Then for the Walk-up it can be more impulsive like, go to buy some stuff/food and then come back quickly so that can explain the Round Trip section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4)__ Considering only trips that begin in the morning hours (before noon), plot in a single bar chart the proportion of trips by passholder type and trip route category. Explain any outliers you observe.\n",
    "\n",
    "For this query we only want the data with a 'Start Time' with a value less than 12 (noon). \n",
    "We first have to do a condition and store the new extracted data to be more clear.\n",
    "\n",
    "We then want to observe again the proportion for Route Category and Passholder Type so we have to create a new crosstab to have the number of occurences for each attributes. This time we do not want the data to be normalized. Finally print using the 'bar' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter and store the data_bikes to only get the trips starting before noon\n",
    "data_bikes_filtered_morning = data_bikes[data_bikes['Start Time'].dt.hour < 12]\n",
    "\n",
    "# Creation of the cross tab to get the number of occurences with \"Passholder Type\" & \"Trip Route Category\"\n",
    "cross_tab_triproute_pass_filtered = pd.crosstab(data_bikes_filtered_morning[\"Passholder Type\"], data_bikes_filtered_morning[\"Trip Route Category\"])\n",
    "\n",
    "#Print to have a quick view on the data and its value\n",
    "print(cross_tab_triproute_pass_filtered)\n",
    "\n",
    "# Plot the extracted and counted data\n",
    "cross_tab_triproute_pass_filtered.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotted and printed data is showing some information. There is much more 'One Way' trips than 'Round Trip'. We can explain this, as people usually want to use bike to go to another place and to be faster than by walk. Additionally, this bar is showing data from the morning part, so these trips could be the way to go to work.\n",
    "We can also observe that there is nearly no Staff Annual Passholder Type. Indeed these are exceptionnals Passholder type. The monthly pass is in contrary the most used Passholder, but is weirdly not the most used for the 'Round Trip' category.  It is the same for the FlexPass, and this can be explain by the fact, that if someone wants a specific Membership (Passholder) it probably means that they want to use it for some regular travels, to go in some specific place (work, home) and not for some quick travel (round trip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5)__ Separate the hours of the day into two intervals that have (approximately) the same number of bikes leaving the stations. For each of the two intervals calculate the proportion of trips by passholder type and trip route category. Present your results in a DataFrame which has a unique, non-composite index. \n",
    "\n",
    "\n",
    "To split the data into 2 parts with the same number of bikes leaving the stations we have to find the index using the histograms data. So we will use the histogram made in the __2) Hour Time Histogram__ part. To find the index we will add the value of each bin in the histogram (counting the number of trips per hour) and divide by the total number of trips to finally print the percentage. We will then choose the best index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for indice to split in 2 parts\n",
    "batchsum = 0\n",
    "for i in range(len(hist_starttime)):\n",
    "    #Add the value of each bin's value\n",
    "    batchsum = batchsum + hist_starttime[i]\n",
    "    #Print the index and the computed percentage of trips\n",
    "    print(str(i) + \"  :   \" + str(batchsum/np.sum(hist_starttime)))\n",
    "    \n",
    "\n",
    "#The best Index is 15. We calculate the ratio to see that in each part there is approximately 50% of the trips.\n",
    "print(\"\\nIndex 15 :\")\n",
    "print(\"1st interval : \"+str(np.sum(hist_starttime[:15])/np.sum(hist_starttime)))\n",
    "print(\"2nd interval : \"+str(np.sum(hist_starttime[15:])/np.sum(hist_starttime)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see on this printed computations that the best index to split the dataset in 2 \"equal\" parts is the __index 15__.\n",
    "\n",
    "\n",
    "Now for each of the two intervals we want to calculate the proportion of trips by passholder type and trip route category. First of all we have to extract in 2 DataFrame the filtered data : First part has the hour of 'Start Time' which is less than 15, and the second part is greater than 15. \n",
    "\n",
    "Then to compute the proportion we create 2 crosstab merging the 2 attributes \"Trip Route Category\" and \"Passholder Type\". Finally we want to merge these 2 DataFrame into a single one, with a unique index. There is multiple solutions to create a unique index, but to have a clear one we created a vector of 4 elements, describing 'One Way1', 'Round Trip1' for the Interval 1 and 'One Way2', 'Round Trip2' for the Interval 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and store the data to extract only with hour < 15\n",
    "starttime_1 = data_bikes[data_bikes['Start Time'].dt.hour < 15]\n",
    "# Filter and store the data to extract only with hour >= 15\n",
    "starttime_2 = data_bikes[data_bikes['Start Time'].dt.hour >= 15]\n",
    "\n",
    "# Create crosstab to get the proportion (Trip Route Cat. and Passholder Type) for the 2 intervals\n",
    "cross1 = pd.crosstab(starttime_1[\"Trip Route Category\"], starttime_1[\"Passholder Type\"])\n",
    "cross2 = pd.crosstab(starttime_2[\"Trip Route Category\"], starttime_2[\"Passholder Type\"])\n",
    "\n",
    "# Concatenate the 2 intervals into a single one\n",
    "result_dataframe = pd.concat([cross1,cross2])\n",
    "\n",
    "# Transform the index into a unique one\n",
    "result_dataframe.index = ['One Way1', 'Round Trip1', 'One Way2', 'Round Trip2']\n",
    "print(result_dataframe)\n",
    "\n",
    "#Check that the index is unique\n",
    "print(result_dataframe.index.is_unique)\n",
    "\n",
    "# Plot the merged DataFrame\n",
    "result_dataframe.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the proportion of trips depend on whether it is the first or second hour interval? Would the company have any significant benefit by creating a more complex paying scheme where monthly pass users would pay less in the first interval and (equally) more on the second one? Assume that the number of trips per interval will not change if the scheme changes.\n",
    "\n",
    "We can observe that the proportion is approximately the same whether it is in the 1st interval or in the 2nd interval. We can also tell that the company won't have any signiciant benefit by creating a new scheme where the users pay less in the first interval because the proportion are equal. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
